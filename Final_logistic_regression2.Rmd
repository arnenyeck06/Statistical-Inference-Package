
# Logistic Regression Model for Gestational Diabetes Prediction

```{r}
library(caret)
library(dplyr)
library(purrr)
library(pROC)
library(yardstick)

# Read the data
#data <- read.csv("completed_data_rf 1.csv")

data <- read.csv("https://raw.githubusercontent.com/arnenyeck06/Statistical-Inference-Package/main/completed_data_rf.csv")

# Feature columns
feature_cols <- c("Pregnancies", "Glucose", "BloodPressure", "SkinThickness", 
                  "Insulin", "BMI", "Pedigree", "Age")

# Count outliers per feature
count_outliers <- function(x) {
  Q1 <- quantile(x, 0.25, na.rm = TRUE)
  Q3 <- quantile(x, 0.75, na.rm = TRUE)
  IQR_val <- Q3 - Q1
  sum(x < (Q1 - 1.5 * IQR_val) | x > (Q3 + 1.5 * IQR_val), na.rm = TRUE)
}

cat("Outliers per feature before capping:\n")
sapply(data[feature_cols], count_outliers)

# Cap outliers using IQR method
cap_outliers <- function(x) {
  Q1 <- quantile(x, 0.25, na.rm = TRUE)
  Q3 <- quantile(x, 0.75, na.rm = TRUE)
  IQR_val <- Q3 - Q1
  lower <- Q1 - 1.5 * IQR_val
  upper <- Q3 + 1.5 * IQR_val
  x[x < lower] <- lower
  x[x > upper] <- upper
  return(x)
}

# Apply outlier capping
data[feature_cols] <- lapply(data[feature_cols], cap_outliers)

# Check class distribution
cat("\nClass distribution:\n")
table(data$Diagnosis)

# Prepare data for modeling
data_model <- data
set.seed(123)

# Stratified train-test split (70-30)
train_index <- createDataPartition(data_model$Diagnosis, p = 0.7, list = FALSE)
train_data <- data_model[train_index, ]
test_data <- data_model[-train_index, ]

# Convert target to factor with meaningful labels
train_data$Diagnosis <- factor(train_data$Diagnosis, levels = c(0, 1), labels = c("No", "Yes"))
test_data$Diagnosis <- factor(test_data$Diagnosis, levels = c(0, 1), labels = c("No", "Yes"))

# Check class proportions
cat("\nTraining set proportions:\n")
prop.table(table(train_data$Diagnosis))
cat("\nTest set proportions:\n")
prop.table(table(test_data$Diagnosis))
```

## Data Preprocessing and Standardization

```{r}
# Standardize features AFTER train-test split to avoid data leakage
# Calculate scaling parameters from training data only
preprocess_params <- preProcess(train_data[feature_cols], method = c("center", "scale"))

# Apply standardization to both sets using training parameters
train_scaled <- predict(preprocess_params, train_data[feature_cols])
test_scaled <- predict(preprocess_params, test_data[feature_cols])

# Combine with target variable
train_final <- cbind(train_scaled, Diagnosis = train_data$Diagnosis)
test_final <- cbind(test_scaled, Diagnosis = test_data$Diagnosis)

cat("Data standardization completed using training set parameters\n")
```

## Cross-Validation Setup

```{r cv_setup}
# Set up 10-fold cross-validation with SMOTE for class imbalance
ctrl <- trainControl(
  method = "cv",
  number = 10,
  classProbs = TRUE,
  summaryFunction = twoClassSummary,
  savePredictions = TRUE,
  sampling = "smote"  # Handle class imbalance
)
```

## Default Logistic Regression Model

```{r}
# Train default logistic regression model
lr_model_default <- train(
  Diagnosis ~ ., 
  data = train_final,
  method = "glm",
  family = "binomial",
  trControl = ctrl,
  metric = "ROC"
)

print(lr_model_default)

# Cross-validation performance by fold
preds <- lr_model_default$pred
fold_list <- split(preds, preds$Resample)
fold_metrics <- map_dfr(fold_list, function(fold) {
  # Create confusion matrix for each fold
  conf_fold <- confusionMatrix(fold$pred, fold$obs, positive = "Yes")
  
  data.frame(
    Resample = unique(fold$Resample),
    ROC = as.numeric(roc(fold$obs, fold$Yes, quiet = TRUE)$auc),
    Sens = conf_fold$byClass["Sensitivity"],
    Spec = conf_fold$byClass["Specificity"]
  )
})

cat("\nCross-validation performance by fold:\n")
print(fold_metrics)

cat("\nMean cross-validation performance:\n")
fold_metrics %>%
  summarise(
    Mean_ROC = mean(ROC),
    Mean_Sens = mean(Sens),
    Mean_Spec = mean(Spec)
  ) %>%
  print()
```

## Model Evaluation on Test Set

```{r}
# Predictions on test set
test_pred_class <- predict(lr_model_default, newdata = test_final)
test_pred_prob <- predict(lr_model_default, newdata = test_final, type = "prob")[, "Yes"]

# Confusion matrix
conf_mat <- confusionMatrix(test_pred_class, test_final$Diagnosis, positive = "Yes")
print(conf_mat)

# Calculate additional metrics
TP <- conf_mat$table[2, 2]
FP <- conf_mat$table[2, 1]
FN <- conf_mat$table[1, 2]

precision <- if ((TP + FP) == 0) NA else TP / (TP + FP)
recall <- if ((TP + FN) == 0) NA else TP / (TP + FN)
f1 <- if (is.na(precision) | is.na(recall) | (precision + recall == 0)) NA else 2 * (precision * recall) / (precision + recall)
f2 <- if (is.na(precision) | is.na(recall) | ((4 * precision) + recall == 0)) NA else (5 * precision * recall) / ((4 * precision) + recall)

# ROC curve
roc_val <- tryCatch({
  pROC::roc(test_final$Diagnosis, test_pred_prob, quiet = TRUE)$auc
}, error = function(e) NA)

cat("\nAdditional Test Set Metrics:\n")
cat("Precision: ", round(precision, 4), "\n")
cat("F1 Score : ", round(f1, 4), "\n")
cat("F2 Score : ", round(f2, 4), "\n")
cat("ROC AUC  : ", round(roc_val, 4), "\n")
```

## Feature Importance and Model Interpretation

```{r}
# Extract model coefficients for interpretation
model_summary <- summary(lr_model_default$finalModel)
coefficients <- model_summary$coefficients

cat("\nLogistic Regression Coefficients:\n")
print(round(coefficients, 4))

# Calculate odds ratios
odds_ratios <- exp(coefficients[, "Estimate"])
cat("\nOdds Ratios:\n")
print(round(odds_ratios, 4))

# Feature importance based on absolute coefficient values
feature_importance <- abs(coefficients[-1, "Estimate"])  # Exclude intercept
feature_importance <- sort(feature_importance, decreasing = TRUE)

cat("\nFeature Importance (based on absolute coefficients):\n")
print(round(feature_importance, 4))
```

## Model Performance Summary

```{r}
cat("\n=== LOGISTIC REGRESSION MODEL SUMMARY ===\n")

cat("\nCross-Validation Performance:\n")
cat("Mean ROC AUC: ", round(mean(fold_metrics$ROC), 4), "\n")
cat("Mean Sensitivity: ", round(mean(fold_metrics$Sens), 4), "\n")
cat("Mean Specificity: ", round(mean(fold_metrics$Spec), 4), "\n")

cat("\nTest Set Performance:\n")
cat("ROC AUC: ", round(roc_val, 4), "\n")
cat("Accuracy: ", round(conf_mat$overall["Accuracy"], 4), "\n")
cat("Sensitivity: ", round(conf_mat$byClass["Sensitivity"], 4), "\n")
cat("Specificity: ", round(conf_mat$byClass["Specificity"], 4), "\n")
cat("Precision: ", round(precision, 4), "\n")
cat("F1 Score: ", round(f1, 4), "\n")

cat("\nTop 3 Most Important Features:\n")
top_features <- names(head(feature_importance, 3))
for(i in 1:3) {
  cat(i, ". ", top_features[i], ": ", round(feature_importance[top_features[i]], 4), "\n")
}
```