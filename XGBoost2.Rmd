---
output:
  word_document: default

---

###Model Selection XGBoost
```{r setup, message=FALSE, warning=FALSE}
library(caret)
library(dplyr)
library(purrr)
library(pROC)
library(xgboost)
library(lime)
library(DescTools)


# Read files
data <- read.csv("https://raw.githubusercontent.com/arnenyeck06/Statistical-Inference-Package/main/completed_data_rf.csv")

feature_cols <- c("Pregnancies", "Glucose", "BloodPressure", "SkinThickness",
                  "Insulin", "BMI", "Pedigree", "Age")

# Count outliers
count_outliers <- function(x) {
  Q1 <- quantile(x, 0.25, na.rm = TRUE)
  Q3 <- quantile(x, 0.75, na.rm = TRUE)
  IQR_val <- Q3 - Q1
  sum(x < (Q1 - 1.5 * IQR_val) | x > (Q3 + 1.5 * IQR_val), na.rm = TRUE)
}
sapply(data[feature_cols], count_outliers)

# Cap outliers
cap_outliers <- function(x) {
  Q1 <- quantile(x, 0.25, na.rm = TRUE)
  Q3 <- quantile(x, 0.75, na.rm = TRUE)
  IQR_val <- Q3 - Q1
  lower <- Q1 - 1.5 * IQR_val
  upper <- Q3 + 1.5 * IQR_val
  x[x < lower] <- lower
  x[x > upper] <- upper
  return(x)
}
data[feature_cols] <- lapply(data[feature_cols], cap_outliers)

## Class imbalance
table(data$Diagnosis)

# Model preparation
data1 <- data
set.seed(123)
train_index <- createDataPartition(data1$Diagnosis, p = 0.70, list = FALSE)
train_data <- data1[train_index, ]
test_data <- data1[-train_index, ]

train_data$Diagnosis <- factor(train_data$Diagnosis, levels = c(0, 1), labels = c("No", "Yes"))
test_data$Diagnosis <- factor(test_data$Diagnosis, levels = c(0, 1), labels = c("No", "Yes"))

# Cross-validation setup
ctrl <- trainControl(
  method = "cv",
  number = 10,
  classProbs = TRUE,
  summaryFunction = twoClassSummary,
  savePredictions = TRUE,
  sampling = "smote"  
)

# Default model
suppressMessages(
  suppressWarnings({
    xgb_model_default <- train(
      Diagnosis ~ ., 
      data = train_data,
      method = "xgbTree",
      trControl = ctrl,
      metric = "ROC"
    )
  })
)

# Fold performance
preds <- xgb_model_default$pred
fold_list <- split(preds, preds$Resample)
fold_metrics <- map_dfr(fold_list, function(fold) {
  preds <- factor(fold$pred, levels = c("No", "Yes"))
  truth <- factor(fold$obs, levels = c("No", "Yes"))
  
  # Create confusion matrix
  cm <- caret::confusionMatrix(preds, truth, positive = "Yes")
  
  data.frame(
    Resample = unique(fold$Resample),
    ROC  = as.numeric(pROC::roc(truth, fold$Yes, quiet = TRUE)$auc),
    Sens = cm$byClass["Sensitivity"],
    Spec = cm$byClass["Specificity"]
  )
}, .id = NULL)
print(fold_metrics)

cat("\\nAveraged over 10 folds:\\n")
fold_metrics %>%
  summarise(
    Mean_ROC = mean(ROC),
    Mean_Sens = mean(Sens),
    Mean_Spec = mean(Spec)
  ) %>%
  print()

# Evaluation on test set
test_pred_class <- predict(xgb_model_default, newdata = test_data)
test_pred_prob <- predict(xgb_model_default, newdata = test_data, type = "prob")[, "Yes"]
conf_mat <- confusionMatrix(test_pred_class, test_data$Diagnosis, positive = "Yes")
print(conf_mat)

TP <- conf_mat$table[2, 2]
FP <- conf_mat$table[1, 2]
FN <- conf_mat$table[2, 1]

# Manually compute additional metrics
precision <- if ((TP + FP) == 0) NA else TP / (TP + FP)
recall    <- if ((TP + FN) == 0) NA else TP / (TP + FN)
f1        <- if (is.na(precision) | is.na(recall) | (precision + recall == 0)) NA else 2 * (precision * recall) / (precision + recall)
f2        <- if (is.na(precision) | is.na(recall) | ((4 * precision) + recall == 0)) NA else (5 * precision * recall) / ((4 * precision) + recall)

# Compute ROC
roc_val   <- tryCatch({
  pROC::roc(test_data$Diagnosis, test_pred_prob, quiet = TRUE)$auc
}, error = function(e) NA)

# Print results
cat("\nAdditional Evaluation Metrics on Test Set:\n")
cat("Precision: ", round(precision, 4), "\n")
cat("F1 Score : ", round(f1, 4), "\n")
cat("F2 Score : ", round(f2, 4), "\n")
cat("ROC AUC  : ", round(roc_val, 4), "\n")

# Hyperparameter tuning
xgb_grid <- expand.grid(
  nrounds = c(50, 100),
  max_depth = c(1, 3, 6),
  eta = c(0.01,0.3, 0.1),
  gamma = 0,
  colsample_bytree = c(0.6,0.8,1),
  min_child_weight = 1,
  subsample = c(0.75,1)
)

xgb_model_tuned <- train(
  Diagnosis ~ ., 
  data = train_data,
  method = "xgbTree",
  metric = "ROC",
  tuneGrid = xgb_grid,
  trControl = ctrl
)


print(xgb_model_tuned$bestTune)

# Tuned fold results
preds_tuned <- xgb_model_tuned$pred
best_params <- xgb_model_tuned$bestTune
preds_best <- preds_tuned %>% filter(
  nrounds == best_params$nrounds,
  max_depth == best_params$max_depth,
  eta == best_params$eta
)
fold_list_tuned <- split(preds_best, preds_best$Resample)
fold_metrics_tuned <- map_dfr(fold_list_tuned, function(fold) {
  preds <- factor(fold$pred, levels = c("No", "Yes"))
  truth <- factor(fold$obs, levels = c("No", "Yes"))
  
  # Create confusion matrix
  cm <- caret::confusionMatrix(preds, truth, positive = "Yes")
  
  data.frame(
    Resample = unique(fold$Resample),
    ROC  = as.numeric(pROC::roc(truth, fold$Yes, quiet = TRUE)$auc),
    Sens = cm$byClass["Sensitivity"],
    Spec = cm$byClass["Specificity"]
  )
}, .id = NULL)
print(fold_metrics_tuned)

# Final test evaluation
test_pred_class_tuned <- predict(xgb_model_tuned, newdata = test_data)
test_pred_prob_tuned <- predict(xgb_model_tuned, newdata = test_data, type = "prob")[, "Yes"]
conf_mat_tuned <- confusionMatrix(test_pred_class_tuned, test_data$Diagnosis, positive = "Yes")
print(conf_mat_tuned)

TP <- conf_mat_tuned$table[2, 2]
FP <- conf_mat_tuned$table[1, 2]
FN <- conf_mat_tuned$table[2, 1]

# Manually compute additional metrics
precision <- if ((TP + FP) == 0) NA else TP / (TP + FP)
recall    <- if ((TP + FN) == 0) NA else TP / (TP + FN)
f1        <- if (is.na(precision) | is.na(recall) | (precision + recall == 0)) NA else 2 * (precision * recall) / (precision + recall)
f2        <- if (is.na(precision) | is.na(recall) | ((4 * precision) + recall == 0)) NA else (5 * precision * recall) / ((4 * precision) + recall)

# Compute ROC
roc_val   <- tryCatch({
  pROC::roc(test_data$Diagnosis, test_pred_prob_tuned, quiet = TRUE)$auc
}, error = function(e) NA)

# Print results
cat("\nAdditional Evaluation Metrics on Test Set:\n")
cat("Precision: ", round(precision, 4), "\n")
cat("F1 Score : ", round(f1, 4), "\n")
cat("F2 Score : ", round(f2, 4), "\n")
cat("ROC AUC  : ", round(roc_val, 4), "\n")

# Manually set a new threshold 
threshold <- 0.45

# Apply threshold to calibrated probabilities
preds_class_tuned <- ifelse(test_pred_prob_tuned >= threshold, "Yes", "No")
preds_class_tuned <- factor(preds_class_tuned, levels = c("No", "Yes"))

# Confusion matrix with new threshold
conf_mat_thresh <- confusionMatrix(preds_class_tuned, test_data$Diagnosis, positive = "Yes")
print(conf_mat_thresh)

# Manually compute metrics
TP <- conf_mat_thresh$table[2, 2]
FP <- conf_mat_thresh$table[1, 2]
FN <- conf_mat_thresh$table[2, 1]

precision <- if ((TP + FP) == 0) NA else TP / (TP + FP)
recall    <- if ((TP + FN) == 0) NA else TP / (TP + FN)
f1        <- if (is.na(precision) | is.na(recall) | (precision + recall == 0)) NA else 2 * (precision * recall) / (precision + recall)
f2        <- if (is.na(precision) | is.na(recall) | ((4 * precision) + recall == 0)) NA else (5 * precision * recall) / ((4 * precision) + recall)

# Compute ROC AUC
roc_val <- pROC::roc(test_data$Diagnosis, test_pred_prob_tuned, quiet = TRUE)$auc

cat("\nEvaluation with Threshold =", threshold, "\n")
cat("Precision: ", round(precision, 4), "\n")
cat("Recall   : ", round(recall, 4), "\n")
cat("F1 Score : ", round(f1, 4), "\n")
cat("F2 Score : ", round(f2, 4), "\n")
cat("ROC AUC  : ", round(roc_val, 4), "\n")



```
```{r lime_plot, fig.width=12, fig.height=20}
# Get final model for LIME
final_model <- xgb_model_tuned$finalModel

# Remove target variable
test_x <- test_data %>% select(-Diagnosis)

# LIME requires the training data used for explainer training
train_x <- train_data %>% select(-Diagnosis)

# Create explainer object
test_instances <- test_data[1:3, feature_cols]
set.seed(123)
explainer <- lime::lime(
  x = train_data[, feature_cols],
  model = xgb_model_tuned,
  bin_continuous = TRUE
)

set.seed(123) 
explanation <- lime::explain(
    test_x[3:5, ],
     explainer = explainer,
     n_features = 8,
     labels = "Yes"
 )

p <- plot_features(explanation)

# Increase text sizes
p + theme(
  text = element_text(size = 14),        
  axis.text = element_text(size = 12),   
  axis.title = element_text(size = 14),  
  legend.text = element_text(size = 12), 
  legend.title = element_text(size = 13),
  strip.text = element_text(size = 13)   
)

# Manually set a new threshold (e.g., 0.4)
threshold <- 0.45

# Apply threshold to calibrated probabilities
preds_class_tuned <- ifelse(test_pred_prob_tuned >= threshold, "Yes", "No")
preds_class_tuned <- factor(preds_class_tuned, levels = c("No", "Yes"))

# Confusion matrix with new threshold
conf_mat_thresh <- confusionMatrix(preds_class_tuned, test_data$Diagnosis, positive = "Yes")
print(conf_mat_thresh)

# Manually compute metrics
TP <- conf_mat_thresh$table[2, 2]
FP <- conf_mat_thresh$table[1, 2]
FN <- conf_mat_thresh$table[2, 1]

precision <- if ((TP + FP) == 0) NA else TP / (TP + FP)
recall    <- if ((TP + FN) == 0) NA else TP / (TP + FN)
f1        <- if (is.na(precision) | is.na(recall) | (precision + recall == 0)) NA else 2 * (precision * recall) / (precision + recall)
f2        <- if (is.na(precision) | is.na(recall) | ((4 * precision) + recall == 0)) NA else (5 * precision * recall) / ((4 * precision) + recall)

# Compute ROC AUC
roc_val <- pROC::roc(test_data$Diagnosis, test_pred_prob_tuned, quiet = TRUE)$auc

cat("\nEvaluation with Threshold =", threshold, "\n")
cat("Precision: ", round(precision, 4), "\n")
cat("Recall   : ", round(recall, 4), "\n")
cat("F1 Score : ", round(f1, 4), "\n")
cat("F2 Score : ", round(f2, 4), "\n")
cat("ROC AUC  : ", round(roc_val, 4), "\n")


```